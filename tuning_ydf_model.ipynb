{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T21:28:18.970836Z",
     "start_time": "2025-03-28T21:28:18.636756Z"
    }
   },
   "cell_type": "code",
   "source": "!.venv/bin/pip3 install -r requirements.txt",
   "id": "45ae6082f3f0015d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;31merror\u001B[0m: \u001B[1mexternally-managed-environment\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m This environment is externally managed\r\n",
      "\u001B[31m╰─>\u001B[0m To install Python packages system-wide, try apt install\r\n",
      "\u001B[31m   \u001B[0m python3-xyz, where xyz is the package you are trying to\r\n",
      "\u001B[31m   \u001B[0m install.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Debian-packaged Python package,\r\n",
      "\u001B[31m   \u001B[0m create a virtual environment using python3 -m venv path/to/venv.\r\n",
      "\u001B[31m   \u001B[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n",
      "\u001B[31m   \u001B[0m sure you have python3-full installed.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Debian packaged Python application,\r\n",
      "\u001B[31m   \u001B[0m it may be easiest to use pipx install xyz, which will manage a\r\n",
      "\u001B[31m   \u001B[0m virtual environment for you. Make sure you have pipx installed.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m See /usr/share/doc/python3.12/README.venv for more information.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001B[1;36mhint\u001B[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T22:57:03.716160Z",
     "start_time": "2025-03-28T22:57:02.413722Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers\n",
    "from keras.src.layers import Lambda\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import ydf  # Yggdrasil Decision Forests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wurlitzer import sys_pipes\n",
    "import keras.layers as preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import tqdm as tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 10]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 18:57:02.576524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743202622.587200   19209 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743202622.590472   19209 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743202622.599574   19209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743202622.599582   19209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743202622.599583   19209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743202622.599584   19209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-28 18:57:02.602837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T22:57:06.065257Z",
     "start_time": "2025-03-28T22:57:06.006767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to avoid TensorFlow using all GPU memory\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        # Set TensorFlow to use only the first GPU (if multiple GPUs available)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        print(\"Using GPU:\", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "id": "1cbeeb19f955c7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T22:57:20.118517Z",
     "start_time": "2025-03-28T22:57:20.112327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_eval(model, train_ds, test_ds = None):\n",
    "    # Optionally, add evaluation metrics.\n",
    "    model.compile(metrics=[\"mse\"])\n",
    "    rmse = 0\n",
    "\n",
    "    with sys_pipes():\n",
    "        model.fit(x=train_ds)\n",
    "\n",
    "    if test_ds is not None:\n",
    "        evaluation = model.evaluate(x=test_ds, return_dict=True)\n",
    "        rmse = math.sqrt(evaluation[\"mse\"])\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def latlon_to_xyz(lat, lon):\n",
    "    lat, lon = np.radians(lat), np.radians(lon)\n",
    "    x = np.cos(lat) * np.cos(lon)\n",
    "    y = np.cos(lat) * np.sin(lon)\n",
    "    z = np.sin(lat)\n",
    "    return x, y, z\n",
    "\n",
    "# Example: Normalize Cartesian coordinates between 0 and 1\n",
    "def normalize_xyz(x, y, z):\n",
    "    # Normalizing each coordinate to the [0, 1] range\n",
    "    return (x + 1) / 2, (y + 1) / 2, (z + 1) / 2"
   ],
   "id": "6c9c236a46c60459",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:13:11.120674Z",
     "start_time": "2025-03-28T23:13:11.080638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROOT_DIR = \"temporary\"\n",
    "train_df = pd.read_csv(f'{ROOT_DIR}/Train.csv')\n",
    "test_df = pd.read_csv(f'{ROOT_DIR}/Test.csv')\n",
    "vocab_df = pd.read_csv(f'{ROOT_DIR}/variable_descriptions.csv')\n",
    "admin_df = pd.read_csv(f'{ROOT_DIR}/zaf_adminboundaries_tabulardata.csv', sep=\";\")\n",
    "\n",
    "admin_df = admin_df[[\"ADM4_PCODE\", \"AREA_SQKM\", \"ADM2_ID\"]] # ADM3_ID\n",
    "admin_df[\"AREA_SQKM\"] = admin_df[\"AREA_SQKM\"].str.replace(\",\", \".\").astype(float)\n",
    "train_df = pd.merge(train_df, admin_df, on=\"ADM4_PCODE\", how=\"left\")\n",
    "test_df = pd.merge(test_df, admin_df, on=\"ADM4_PCODE\", how=\"left\")\n",
    "label_column = \"target\"\n",
    "\n",
    "default_columns = [\"ward\", \"ADM4_PCODE\"]\n",
    "nul_cols = [\"dw_12\", \"dw_13\", \"lan_13\", \"pw_08\", \"pw_07\"] # Columns with null values\n",
    "cat_columns = [\"ADM2_ID\"] # Categorical columns\n",
    "ft_columns = default_columns + cat_columns"
   ],
   "id": "baaa89ce9b2530a6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:06:53.039595Z",
     "start_time": "2025-03-29T00:06:53.034863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# categorical_encoder = OneHotEncoder(sparse_output=False)\n",
    "def preprocess_df(input_df, is_train=False):\n",
    "    drop_cols = []\n",
    "    df = input_df.copy()\n",
    "    df = df.drop(nul_cols, axis=1)\n",
    "\n",
    "    ## Create a new feature\n",
    "    df[\"phi\"] = df[\"total_individuals\"] / df[\"total_households\"]\n",
    "    df[\"id_area\"] = df[\"total_individuals\"] / df[\"AREA_SQKM\"]\n",
    "    df[\"hs_area\"] = df[\"total_households\"] / df[\"AREA_SQKM\"]\n",
    "    \n",
    "    ## Normalize some columns\n",
    "    norm_cols = [\"phi\", \"NL\", \"id_area\", \"hs_area\"]\n",
    "    df[norm_cols] = scaler.fit_transform(df[norm_cols]) if is_train else scaler.transform(df[norm_cols])\n",
    "\n",
    "    ## Encode categorical columns\n",
    "    # encoded = categorical_encoder.fit_transform(df[cat_columns]) if is_train else categorical_encoder.transform(df[cat_columns])\n",
    "    # encoded_df = pd.DataFrame(encoded, columns=categorical_encoder.get_feature_names_out(cat_columns))\n",
    "    # df = pd.concat([df, encoded_df], axis=1)\n",
    "    drop_cols = drop_cols + cat_columns\n",
    "    \n",
    "    ## Transform lat and lon to xyz\n",
    "    df[\"x\"], df[\"y\"], df[\"z\"] = zip(*df.apply(lambda x: latlon_to_xyz(x[\"lat\"], x[\"lon\"]), axis=1))\n",
    "    df[\"x\"], df[\"y\"], df[\"z\"] = zip(*df.apply(lambda x: normalize_xyz(x[\"x\"], x[\"y\"], x[\"z\"]), axis=1))\n",
    "\n",
    "    drop_cols = drop_cols + default_columns\n",
    "    if len(drop_cols) > 0:\n",
    "        df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Test preprocess_df\n",
    "# preprocess_df(train_ds_pd, is_train=True).head()"
   ],
   "id": "1359c575af7915c5",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test with categorical columns",
   "id": "2ef8057666349c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:11:28.420574Z",
     "start_time": "2025-03-28T23:11:28.320540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = preprocess_df(X_train, is_train=True)\n",
    "X_val = preprocess_df(X_val, is_train=False)\n",
    "train_ds_pd = pd.concat([X_train, y_train], axis=1)\n",
    "test_ds_pd = pd.concat([X_val, y_val], axis=1)"
   ],
   "id": "2a462bfe90cdaf9e",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['DC39', 'DC45', 'DC38', 'DC8', 'DC7', 'DC6', 'DC40', 'DC9', 'DC37'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m X_train = preprocess_df(X_train, is_train=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m X_val = \u001B[43mpreprocess_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m train_ds_pd = pd.concat([X_train, y_train], axis=\u001B[32m1\u001B[39m)\n\u001B[32m      4\u001B[39m test_ds_pd = pd.concat([X_val, y_val], axis=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mpreprocess_df\u001B[39m\u001B[34m(input_df, is_train)\u001B[39m\n\u001B[32m     17\u001B[39m df[norm_cols] = scaler.fit_transform(df[norm_cols]) \u001B[38;5;28;01mif\u001B[39;00m is_train \u001B[38;5;28;01melse\u001B[39;00m scaler.transform(df[norm_cols])\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m## Encode categorical columns\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m encoded = categorical_encoder.fit_transform(df[cat_columns]) \u001B[38;5;28;01mif\u001B[39;00m is_train \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mcategorical_encoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcat_columns\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m encoded_df = pd.DataFrame(encoded, columns=categorical_encoder.get_feature_names_out(cat_columns))\n\u001B[32m     22\u001B[39m df = pd.concat([df, encoded_df], axis=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    322\u001B[39m         return_tuple = (\n\u001B[32m    323\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    324\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    325\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:1043\u001B[39m, in \u001B[36mOneHotEncoder.transform\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m   1038\u001B[39m     warn_on_unknown = \u001B[38;5;28mself\u001B[39m.drop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handle_unknown \u001B[38;5;129;01min\u001B[39;00m {\n\u001B[32m   1039\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1040\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minfrequent_if_exist\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1041\u001B[39m     }\n\u001B[32m   1042\u001B[39m     handle_unknown = \u001B[38;5;28mself\u001B[39m.handle_unknown\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m X_int, X_mask = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1044\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1045\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1047\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwarn_on_unknown\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwarn_on_unknown\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1050\u001B[39m n_samples, n_features = X_int.shape\n\u001B[32m   1052\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._drop_idx_after_grouping \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:218\u001B[39m, in \u001B[36m_BaseEncoder._transform\u001B[39m\u001B[34m(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\u001B[39m\n\u001B[32m    213\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m handle_unknown == \u001B[33m\"\u001B[39m\u001B[33merror\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    214\u001B[39m     msg = (\n\u001B[32m    215\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFound unknown categories \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m in column \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    216\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m during transform\u001B[39m\u001B[33m\"\u001B[39m.format(diff, i)\n\u001B[32m    217\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    220\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m warn_on_unknown:\n",
      "\u001B[31mValueError\u001B[39m: Found unknown categories ['DC39', 'DC45', 'DC38', 'DC8', 'DC7', 'DC6', 'DC40', 'DC9', 'DC37'] in column 0 during transform"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:06:25.352207Z",
     "start_time": "2025-03-28T23:06:25.349092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Check if there are some ADM2_ID values in the test set that are not in the train set\n",
    "train_zones = set(train_df[\"ADM2_ID\"].unique())\n",
    "test_zones = set(test_df[\"ADM2_ID\"].unique())\n",
    "missing_zones = test_zones - train_zones\n",
    "if len(missing_zones) > 0:\n",
    "    print(f\"Missing zones: {missing_zones}\")"
   ],
   "id": "4b0cb2dc52c2c210",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing zones: {'DC10', 'DC1', 'DC12', 'DC14', 'CPT', 'DC5', 'BUF', 'DC13', 'DC15', 'NMA', 'DC3', 'DC2', 'DC4', 'DC44'}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Can't use column ADM2_ID as a feature because it is not the same in the train and test set.",
   "id": "df9b827156975b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:07:07.559667Z",
     "start_time": "2025-03-29T00:07:07.503095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = train_df.drop(label_column, axis=1)\n",
    "y = train_df[label_column]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = preprocess_df(X_train, is_train=True)\n",
    "X_val = preprocess_df(X_val, is_train=False)\n",
    "train_ds_pd = pd.concat([X_train, y_train], axis=1)\n",
    "test_ds_pd = pd.concat([X_val, y_val], axis=1)"
   ],
   "id": "b36302a08efd75c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:07:09.886955Z",
     "start_time": "2025-03-29T00:07:08.937168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Use ydf instead of tfdf\n",
    "gbt_model = ydf.GradientBoostedTreesLearner(label=label_column, task=ydf.Task.REGRESSION, num_threads=16).train(train_ds_pd)\n",
    "evaluation = gbt_model.evaluate(test_ds_pd)\n",
    "print(evaluation)"
   ],
   "id": "587c27410c7ca3d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2257 examples\n",
      "Model trained in 0:00:00.936360\n",
      "RMSE: 3.85796\n",
      "num examples: 565\n",
      "num examples (weighted): 565\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:16:35.920633Z",
     "start_time": "2025-03-28T23:16:35.917087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bhp_gbt_templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()\n",
    "bhp_gbt_templates"
   ],
   "id": "dfd8ae0119f66a28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better_defaultv1': HyperparameterTemplate(name='better_default', version=1, parameters={'growing_strategy': 'BEST_FIRST_GLOBAL'}, description='A configuration that is generally better than the default parameters without being more expensive.'),\n",
       " 'benchmark_rank1v1': HyperparameterTemplate(name='benchmark_rank1', version=1, parameters={'growing_strategy': 'BEST_FIRST_GLOBAL', 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}, description='Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.')}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:08:01.240127Z",
     "start_time": "2025-03-29T00:07:59.767289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gbt_model = ydf.GradientBoostedTreesLearner(label=label_column, task=ydf.Task.REGRESSION, num_threads=16, **bhp_gbt_templates[\"benchmark_rank1v1\"]).train(train_ds_pd)\n",
    "evaluation = gbt_model.evaluate(test_ds_pd)\n",
    "print(evaluation)"
   ],
   "id": "26126948b054dd42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2257 examples\n",
      "Model trained in 0:00:01.453353\n",
      "RMSE: 4.08855\n",
      "num examples: 565\n",
      "num examples (weighted): 565\n",
      "\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:08:05.837672Z",
     "start_time": "2025-03-29T00:08:04.948750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gbt_model = ydf.GradientBoostedTreesLearner(label=label_column, task=ydf.Task.REGRESSION, num_threads=16, **bhp_gbt_templates[\"better_defaultv1\"])\n",
    "trainer = gbt_model.train(train_ds_pd)\n",
    "evaluation = trainer.evaluate(test_ds_pd)\n",
    "print(evaluation)"
   ],
   "id": "d8999d315c0d81ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2257 examples\n",
      "Model trained in 0:00:00.874679\n",
      "RMSE: 3.93821\n",
      "num examples: 565\n",
      "num examples (weighted): 565\n",
      "\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:35:51.925817Z",
     "start_time": "2025-03-28T23:35:51.922140Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation.rmse, evaluation.characteristics",
   "id": "3222879787fe23a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9379325648555428, None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- YDF achieved a RMSE of 3.87619. Let's try to tune the model. The api is also simpler than the one from TensorFlow Decision Forests.\n",
    "- Using predefined hyperparameters, doesn't seem to improve the model. Let's try to tune the model."
   ],
   "id": "55514ba98c56c163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:31:50.474051Z",
     "start_time": "2025-03-28T23:28:13.249766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_models = {\n",
    "    \"RF\": ydf.RandomForestLearner,\n",
    "    \"GBM\": ydf.GradientBoostedTreesLearner,\n",
    "    \"CT\": ydf.CartLearner,\n",
    "}\n",
    "unused_columns = ft_columns[0]\n",
    "preprocess_function = preprocess_df\n",
    "tuning_logs = {mn: {} for mn in all_models}\n",
    "\n",
    "n_fold = 10\n",
    "kf = KFold(n_splits=n_fold, shuffle=False)\n",
    "hps = []\n",
    "rmse = []\n",
    "# Find best HP\n",
    "X = train_df.drop(label_column, axis=1)\n",
    "y = train_df[label_column]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "for train_index, test_index in tqdm.tqdm(kf.split(X_train), desc=\"Tuning\"):\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    X_fold_train = preprocess_function(X_fold_train, is_train=True)\n",
    "    X_fold_test = preprocess_function(X_fold_test, is_train=False)\n",
    "\n",
    "    X_fold_train[\"target\"] = y_fold_train\n",
    "    X_fold_test[\"target\"] = y_fold_test\n",
    "\n",
    "    for mn, md in all_models.items():\n",
    "        tuner = ydf.RandomSearchTuner(num_trials=50, automatic_search_space=True)\n",
    "        tuned_model = md(label=label_column, task=ydf.Task.REGRESSION, num_threads=16, tuner=tuner)\n",
    "        tuned_trainer = tuned_model.train(X_fold_train)\n",
    "        tuned_evaluation = tuned_model.evaluate(X_fold_test)\n",
    "        rmse = tuned_evaluation.rmse\n",
    "        hp = tuned_model.hyperparameters\n",
    "\n",
    "        model_log = tuning_logs.get(mn, {})\n",
    "        tuning_logs[mn] = {\n",
    "            \"rmse\": model_log.get(\"rmse\", []) + [rmse],\n",
    "            \"hp\": model_log.get(\"hp\", []) + [hp],\n",
    "        }\n"
   ],
   "id": "c0232a2772096f7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2031 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning: 0it [03:37, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/ydf/learner/generic_learner.py:574\u001B[39m, in \u001B[36mGenericCCLearner._train_from_dataset\u001B[39m\u001B[34m(self, ds, valid)\u001B[39m\n\u001B[32m    573\u001B[39m learner = \u001B[38;5;28mself\u001B[39m._get_learner()\n\u001B[32m--> \u001B[39m\u001B[32m574\u001B[39m cc_model = \u001B[43mlearner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtrain_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    575\u001B[39m log.info(\n\u001B[32m    576\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mModel trained in \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    577\u001B[39m     datetime.datetime.now() - time_begin_training_model,\n\u001B[32m    578\u001B[39m )\n",
      "\u001B[31mValueError\u001B[39m: INVALID_ARGUMENT: Operation interrupted by user",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[51]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m tuner = ydf.RandomSearchTuner(num_trials=\u001B[32m50\u001B[39m, automatic_search_space=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     30\u001B[39m tuned_model = md(label=label_column, task=ydf.Task.REGRESSION, num_threads=\u001B[32m16\u001B[39m, tuner=tuner)\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[43mtuned_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_fold_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m rmse = math.sqrt(tuned_model.evaluate(X_fold_test, return_dict=\u001B[38;5;28;01mTrue\u001B[39;00m, verbose=\u001B[32m0\u001B[39m)[\u001B[33m\"\u001B[39m\u001B[33mmse\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     34\u001B[39m tuning_logs = tuned_model.make_inspector().tuning_logs()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/ydf/learner/specialized_learners.py:2879\u001B[39m, in \u001B[36mRandomForestLearner.train\u001B[39m\u001B[34m(self, ds, valid, verbose)\u001B[39m\n\u001B[32m   2836\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain\u001B[39m(\n\u001B[32m   2837\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   2838\u001B[39m     ds: dataset.InputDataset,\n\u001B[32m   2839\u001B[39m     valid: Optional[dataset.InputDataset] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   2840\u001B[39m     verbose: Optional[Union[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mbool\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   2841\u001B[39m ) -> random_forest_model.RandomForestModel:\n\u001B[32m   2842\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Trains a model on the given dataset.\u001B[39;00m\n\u001B[32m   2843\u001B[39m \n\u001B[32m   2844\u001B[39m \u001B[33;03m  Options for dataset reading are given on the learner. Consult the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2877\u001B[39m \u001B[33;03m    A trained model.\u001B[39;00m\n\u001B[32m   2878\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2879\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/ydf/learner/generic_learner.py:344\u001B[39m, in \u001B[36mGenericLearner.train\u001B[39m\u001B[34m(self, ds, valid, verbose)\u001B[39m\n\u001B[32m    342\u001B[39m saved_verbose = log.verbose(verbose) \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m344\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_train_imp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    346\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m saved_verbose \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/ydf/learner/generic_learner.py:495\u001B[39m, in \u001B[36mGenericCCLearner._train_imp\u001B[39m\u001B[34m(self, ds, valid, verbose)\u001B[39m\n\u001B[32m    493\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._train_from_path(ds, valid)\n\u001B[32m    494\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_train_from_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/.venv/lib/python3.12/site-packages/ydf/learner/generic_learner.py:545\u001B[39m, in \u001B[36mGenericCCLearner._train_from_dataset\u001B[39m\u001B[34m(self, ds, valid)\u001B[39m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_train_from_dataset\u001B[39m(\n\u001B[32m    539\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    540\u001B[39m     ds: dataset.InputDataset,\n\u001B[32m    541\u001B[39m     valid: Optional[dataset.InputDataset] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    542\u001B[39m ) -> generic_model.ModelType:\n\u001B[32m    543\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Trains a model from in-memory data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m545\u001B[39m \u001B[43m  \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcc_log_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    546\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_vertical_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_dataset\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[32m    548\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_args\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdataset\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m}\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/contextlib.py:141\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    138\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    139\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mgenerator didn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt yield\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, typ, value, traceback):\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    143\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To obtain accurate HP, we must do a cross-validation to find the best HP. We can use the RMSE as the metric to evaluate the model. But, it takes a long time to run. So, we will only tune the model with the whole training set.",
   "id": "66f9e04685a2adc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:27:19.294676Z",
     "start_time": "2025-03-29T11:45:57.605308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = train_df.drop(label_column, axis=1)\n",
    "y = train_df[label_column]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "preprocess_function = preprocess_df\n",
    "X_train = preprocess_function(X_train, is_train=True)\n",
    "X_val = preprocess_function(X_val, is_train=False)\n",
    "train_ds_pd = pd.concat([X_train, y_train], axis=1)\n",
    "test_ds_pd = pd.concat([X_val, y_val], axis=1)\n",
    "tuning_logs = {mn: {} for mn in all_models}\n",
    "\n",
    "for mn, md in all_models.items():\n",
    "    tuner = ydf.RandomSearchTuner(num_trials=10, automatic_search_space=True)\n",
    "    tuned_model = md(label=label_column, task=ydf.Task.REGRESSION, num_threads=32, tuner=tuner)\n",
    "    tuned_trainer = tuned_model.train(train_ds_pd)\n",
    "    tuned_evaluation = tuned_trainer.evaluate(test_ds_pd)\n",
    "    rmse = tuned_evaluation.rmse\n",
    "    hp = tuned_model.hyperparameters\n",
    "\n",
    "    model_log = tuning_logs.get(mn, {})\n",
    "    tuning_logs[mn] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"hp\": hp,\n",
    "    }\n",
    "\n",
    "tuning_logs"
   ],
   "id": "3c8e25f8498d5755",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2257 examples\n",
      "Model trained in 0:32:12.211291\n",
      "Train model on 2257 examples\n",
      "Model trained in 0:08:34.274344\n",
      "Train model on 2257 examples\n",
      "Model trained in 0:00:34.579165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RF': {'rmse': 3.6206951857773455,\n",
       "  'hp': {'adapt_bootstrap_size_ratio_for_maximum_training_duration': False,\n",
       "   'allow_na_conditions': False,\n",
       "   'bootstrap_size_ratio': 1.0,\n",
       "   'bootstrap_training_dataset': True,\n",
       "   'categorical_algorithm': 'CART',\n",
       "   'categorical_set_split_greedy_sampling': 0.1,\n",
       "   'categorical_set_split_max_num_items': -1,\n",
       "   'categorical_set_split_min_item_frequency': 1,\n",
       "   'compute_oob_performances': True,\n",
       "   'compute_oob_variable_importances': False,\n",
       "   'growing_strategy': 'LOCAL',\n",
       "   'honest': False,\n",
       "   'honest_fixed_separation': False,\n",
       "   'honest_ratio_leaf_examples': 0.5,\n",
       "   'in_split_min_examples_check': True,\n",
       "   'keep_non_leaf_label_distribution': True,\n",
       "   'max_depth': 16,\n",
       "   'max_num_nodes': None,\n",
       "   'maximum_model_size_in_memory_in_bytes': -1.0,\n",
       "   'maximum_training_duration_seconds': -1.0,\n",
       "   'mhld_oblique_max_num_attributes': None,\n",
       "   'mhld_oblique_sample_attributes': None,\n",
       "   'min_examples': 5,\n",
       "   'missing_value_policy': 'GLOBAL_IMPUTATION',\n",
       "   'num_candidate_attributes': 0,\n",
       "   'num_candidate_attributes_ratio': None,\n",
       "   'num_oob_variable_importances_permutations': 1,\n",
       "   'num_trees': 300,\n",
       "   'numerical_vector_sequence_num_examples': 1000,\n",
       "   'numerical_vector_sequence_num_random_anchors': 100,\n",
       "   'pure_serving_model': False,\n",
       "   'random_seed': 123456,\n",
       "   'sampling_with_replacement': True,\n",
       "   'sorting_strategy': 'PRESORT',\n",
       "   'sparse_oblique_max_num_features': None,\n",
       "   'sparse_oblique_max_num_projections': None,\n",
       "   'sparse_oblique_normalization': None,\n",
       "   'sparse_oblique_num_projections_exponent': None,\n",
       "   'sparse_oblique_projection_density_factor': None,\n",
       "   'sparse_oblique_weights': None,\n",
       "   'sparse_oblique_weights_integer_maximum': None,\n",
       "   'sparse_oblique_weights_integer_minimum': None,\n",
       "   'sparse_oblique_weights_power_of_two_max_exponent': None,\n",
       "   'sparse_oblique_weights_power_of_two_min_exponent': None,\n",
       "   'split_axis': 'AXIS_ALIGNED',\n",
       "   'uplift_min_examples_in_treatment': 5,\n",
       "   'uplift_split_score': 'KULLBACK_LEIBLER',\n",
       "   'winner_take_all': True}},\n",
       " 'GBM': {'rmse': 3.6462261807303116,\n",
       "  'hp': {'adapt_subsample_for_maximum_training_duration': False,\n",
       "   'allow_na_conditions': False,\n",
       "   'apply_link_function': True,\n",
       "   'categorical_algorithm': 'CART',\n",
       "   'categorical_set_split_greedy_sampling': 0.1,\n",
       "   'categorical_set_split_max_num_items': -1,\n",
       "   'categorical_set_split_min_item_frequency': 1,\n",
       "   'compute_permutation_variable_importance': False,\n",
       "   'cross_entropy_ndcg_truncation': None,\n",
       "   'dart_dropout': None,\n",
       "   'early_stopping': 'LOSS_INCREASE',\n",
       "   'early_stopping_initial_iteration': 10,\n",
       "   'early_stopping_num_trees_look_ahead': 30,\n",
       "   'focal_loss_alpha': None,\n",
       "   'focal_loss_gamma': None,\n",
       "   'forest_extraction': 'MART',\n",
       "   'goss_alpha': 0.2,\n",
       "   'goss_beta': 0.1,\n",
       "   'growing_strategy': 'LOCAL',\n",
       "   'honest': False,\n",
       "   'honest_fixed_separation': False,\n",
       "   'honest_ratio_leaf_examples': 0.5,\n",
       "   'in_split_min_examples_check': True,\n",
       "   'keep_non_leaf_label_distribution': True,\n",
       "   'l1_regularization': 0.0,\n",
       "   'l2_categorical_regularization': 1.0,\n",
       "   'l2_regularization': 0.0,\n",
       "   'lambda_loss': 1.0,\n",
       "   'loss': 'DEFAULT',\n",
       "   'max_depth': 6,\n",
       "   'max_num_nodes': None,\n",
       "   'maximum_model_size_in_memory_in_bytes': -1.0,\n",
       "   'maximum_training_duration_seconds': -1.0,\n",
       "   'mhld_oblique_max_num_attributes': None,\n",
       "   'mhld_oblique_sample_attributes': None,\n",
       "   'min_examples': 5,\n",
       "   'missing_value_policy': 'GLOBAL_IMPUTATION',\n",
       "   'ndcg_truncation': None,\n",
       "   'num_candidate_attributes': -1,\n",
       "   'num_candidate_attributes_ratio': None,\n",
       "   'num_trees': 300,\n",
       "   'numerical_vector_sequence_num_examples': 1000,\n",
       "   'numerical_vector_sequence_num_random_anchors': 100,\n",
       "   'pure_serving_model': False,\n",
       "   'random_seed': 123456,\n",
       "   'sampling_method': 'RANDOM',\n",
       "   'selective_gradient_boosting_ratio': 0.01,\n",
       "   'shrinkage': 0.1,\n",
       "   'sorting_strategy': 'PRESORT',\n",
       "   'sparse_oblique_max_num_features': None,\n",
       "   'sparse_oblique_max_num_projections': None,\n",
       "   'sparse_oblique_normalization': None,\n",
       "   'sparse_oblique_num_projections_exponent': None,\n",
       "   'sparse_oblique_projection_density_factor': None,\n",
       "   'sparse_oblique_weights': None,\n",
       "   'sparse_oblique_weights_integer_maximum': None,\n",
       "   'sparse_oblique_weights_integer_minimum': None,\n",
       "   'sparse_oblique_weights_power_of_two_max_exponent': None,\n",
       "   'sparse_oblique_weights_power_of_two_min_exponent': None,\n",
       "   'split_axis': 'AXIS_ALIGNED',\n",
       "   'subsample': 1.0,\n",
       "   'uplift_min_examples_in_treatment': 5,\n",
       "   'uplift_split_score': 'KULLBACK_LEIBLER',\n",
       "   'use_hessian_gain': False,\n",
       "   'validation_interval_in_trees': 1,\n",
       "   'validation_ratio': 0.1}},\n",
       " 'CT': {'rmse': 5.290425706159608,\n",
       "  'hp': {'allow_na_conditions': False,\n",
       "   'categorical_algorithm': 'CART',\n",
       "   'categorical_set_split_greedy_sampling': 0.1,\n",
       "   'categorical_set_split_max_num_items': -1,\n",
       "   'categorical_set_split_min_item_frequency': 1,\n",
       "   'growing_strategy': 'LOCAL',\n",
       "   'honest': False,\n",
       "   'honest_fixed_separation': False,\n",
       "   'honest_ratio_leaf_examples': 0.5,\n",
       "   'in_split_min_examples_check': True,\n",
       "   'keep_non_leaf_label_distribution': True,\n",
       "   'max_depth': 16,\n",
       "   'max_num_nodes': None,\n",
       "   'maximum_model_size_in_memory_in_bytes': -1.0,\n",
       "   'maximum_training_duration_seconds': -1.0,\n",
       "   'mhld_oblique_max_num_attributes': None,\n",
       "   'mhld_oblique_sample_attributes': None,\n",
       "   'min_examples': 5,\n",
       "   'missing_value_policy': 'GLOBAL_IMPUTATION',\n",
       "   'num_candidate_attributes': -1,\n",
       "   'num_candidate_attributes_ratio': None,\n",
       "   'numerical_vector_sequence_num_examples': 1000,\n",
       "   'numerical_vector_sequence_num_random_anchors': 100,\n",
       "   'pure_serving_model': False,\n",
       "   'random_seed': 123456,\n",
       "   'sorting_strategy': 'IN_NODE',\n",
       "   'sparse_oblique_max_num_features': None,\n",
       "   'sparse_oblique_max_num_projections': None,\n",
       "   'sparse_oblique_normalization': None,\n",
       "   'sparse_oblique_num_projections_exponent': None,\n",
       "   'sparse_oblique_projection_density_factor': None,\n",
       "   'sparse_oblique_weights': None,\n",
       "   'sparse_oblique_weights_integer_maximum': None,\n",
       "   'sparse_oblique_weights_integer_minimum': None,\n",
       "   'sparse_oblique_weights_power_of_two_max_exponent': None,\n",
       "   'sparse_oblique_weights_power_of_two_min_exponent': None,\n",
       "   'split_axis': 'AXIS_ALIGNED',\n",
       "   'uplift_min_examples_in_treatment': 5,\n",
       "   'uplift_split_score': 'KULLBACK_LEIBLER',\n",
       "   'validation_ratio': 0.1}}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Setting the number of trials to 10, higher values will take a long time to run. I set it to 100 and it run for only the first model for more than 5h.",
   "id": "9cbb91613339c8ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:34:08.076809Z",
     "start_time": "2025-03-29T12:34:08.074126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Print the tuning logs\n",
    "for mn, md in tuning_logs.items():\n",
    "    print(f\"{mn}: Best RMSE: {md['rmse']}\")"
   ],
   "id": "800386d3d76ead5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: Best RMSE: 3.6206951857773455\n",
      "GBM: Best RMSE: 3.6462261807303116\n",
      "CT: Best RMSE: 5.290425706159608\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can have a better model with tuning the hyperparameters.",
   "id": "4cf8eee884444ee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tag_id)",
   "language": "python",
   "name": "tag_id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
