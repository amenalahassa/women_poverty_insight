{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers\n",
    "from keras.src.layers import Lambda\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wurlitzer import sys_pipes\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import keras_tuner as kt\n",
    "import tqdm as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 10]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_and_eval(model, train_ds, test_ds = None):\n",
    "    # Optionally, add evaluation metrics.\n",
    "    model.compile(metrics=[\"mse\"])\n",
    "    rmse = 0\n",
    "\n",
    "    with sys_pipes():\n",
    "        model.fit(x=train_ds)\n",
    "\n",
    "    if test_ds is not None:\n",
    "        evaluation = model.evaluate(x=test_ds, return_dict=True)\n",
    "        rmse = math.sqrt(evaluation[\"mse\"])\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def latlon_to_xyz(lat, lon):\n",
    "    lat, lon = np.radians(lat), np.radians(lon)\n",
    "    x = np.cos(lat) * np.cos(lon)\n",
    "    y = np.cos(lat) * np.sin(lon)\n",
    "    z = np.sin(lat)\n",
    "    return x, y, z\n",
    "\n",
    "# Example: Normalize Cartesian coordinates between 0 and 1\n",
    "def normalize_xyz(x, y, z):\n",
    "    # Normalizing each coordinate to the [0, 1] range\n",
    "    return (x + 1) / 2, (y + 1) / 2, (z + 1) / 2\n",
    "\n",
    "class LatLonToXYZ(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LatLonToXYZ, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        lat, lon = inputs[..., 0], inputs[..., 1]  # Assuming inputs shape is (..., 2)\n",
    "        rad_factor = tf.constant(np.pi / 180, dtype=tf.float32)\n",
    "        lat, lon = lat * rad_factor, lon * rad_factor\n",
    "\n",
    "        x = tf.cos(lat) * tf.cos(lon)\n",
    "        y = tf.cos(lat) * tf.sin(lon)\n",
    "        z = tf.sin(lat)\n",
    "\n",
    "        return tf.stack([x, y, z], axis=-1)\n",
    "\n",
    "class NormalizeXYZ(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NormalizeXYZ, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs + 1) / 2  # Normalize to [0,1] range\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == 'string':\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # TODO\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: tf.compat.as_str_any(x[name]))\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    # return lambda feature: encoder(index(feature))\n",
    "\n",
    "    return Lambda(lambda feature: encoder(index(feature)))\n",
    "\n",
    "def dataset_preprocessing(unused_columns, categorical_ft_columns, X_train):\n",
    "    used_columns = []\n",
    "\n",
    "    raw_inputs = {}\n",
    "    processed_inputs = {}\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        dtype = X_train.dtypes[col]\n",
    "        if col in categorical_ft_columns:\n",
    "            dtype = \"string\"\n",
    "        raw_inputs[col] = tf.keras.layers.Input(shape=(1,), name=col, dtype=dtype)\n",
    "        processed_inputs[col] = raw_inputs[col]\n",
    "\n",
    "    processed_inputs[\"phi\"] = raw_inputs[\"total_individuals\"] / raw_inputs[\"total_households\"]\n",
    "    processed_inputs[\"id_area\"] = raw_inputs[\"total_individuals\"] / raw_inputs[\"AREA_SQKM\"]\n",
    "    processed_inputs[\"hs_area\"] = raw_inputs[\"total_households\"] / raw_inputs[\"AREA_SQKM\"]\n",
    "    lon_lat = tf.keras.layers.Concatenate(name=\"lon_lat\")([raw_inputs[\"lon\"], raw_inputs[\"lat\"]])\n",
    "    processed_inputs[\"xyz\"] = NormalizeXYZ()(LatLonToXYZ()(lon_lat))\n",
    "    zone_encoder = get_category_encoding_layer(\"ADM2_ID\", train_ds, \"string\", 100)\n",
    "    processed_inputs[\"zone\"] = zone_encoder(raw_inputs[\"ADM2_ID\"])\n",
    "    processed_cols = [\"total_individuals\", \"total_households\", \"AREA_SQKM\", \"lon\", \"lat\", \"ADM2_ID\"]\n",
    "    new_cols = [\"phi\", \"id_area\", \"hs_area\", \"xyz\", \"zone\"]\n",
    "\n",
    "    all_features = []\n",
    "    for col in X_train.columns:\n",
    "        if col not in unused_columns + processed_cols:\n",
    "            if col in categorical_ft_columns:\n",
    "                all_features.append(tfdf.keras.FeatureUsage(name=col, semantic=tfdf.keras.FeatureSemantic.CATEGORICAL))\n",
    "            else:\n",
    "                all_features.append(tfdf.keras.FeatureUsage(name=col))\n",
    "            used_columns.append(col)\n",
    "\n",
    "    for col in new_cols:\n",
    "        all_features.append(tfdf.keras.FeatureUsage(name=col))\n",
    "        used_columns.append(col)\n",
    "\n",
    "    preprocessor = tf.keras.Model(inputs=raw_inputs, outputs=processed_inputs, name=\"preprocessor\")\n",
    "\n",
    "    return all_features, preprocessor\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "id": "6c9c236a46c60459"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('outputs/Train.csv')\n",
    "test_df = pd.read_csv('outputs/Test.csv')\n",
    "vocab_df = pd.read_csv('variable_descriptions.csv')\n",
    "admin_df = pd.read_csv('zaf_adminboundaries_tabulardata.csv', sep=\";\")\n",
    "\n",
    "admin_df = admin_df[[\"ADM4_PCODE\", \"AREA_SQKM\", \"ADM2_ID\"]] # ADM3_ID\n",
    "admin_df[\"AREA_SQKM\"] = admin_df[\"AREA_SQKM\"].str.replace(\",\", \".\").astype(float)\n",
    "df = pd.merge(df, admin_df, on=\"ADM4_PCODE\", how=\"left\")\n",
    "test_df = pd.merge(test_df, admin_df, on=\"ADM4_PCODE\", how=\"left\")\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_ds_pd = pd.concat([X_train, y_train], axis=1)\n",
    "test_ds_pd = pd.concat([X_val, y_val], axis=1)\n",
    "label_column = \"target\"\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label_column, task=tfdf.keras.Task.REGRESSION)\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label_column, task=tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "default_columns = [\"ward\", \"ADM4_PCODE\"]\n",
    "nn_cols = [\"dw_00\", \"psa_00\", \"lan_00\", \"pg_00\", \"pw_00\", \"stv_00\", \"car_00\", \"lln_00\"]\n",
    "categorical_ft_columns = [\"ADM2_ID\"]\n",
    "\n",
    "ft_columns = [\n",
    "    default_columns + [\"dw_12\", \"dw_13\", \"lan_13\", \"pw_08\", \"pw_07\"],\n",
    "    default_columns + [\"dw_12\", \"dw_13\", \"lan_13\", \"pw_08\", \"pw_07\"] + nn_cols,\n",
    "    default_columns,\n",
    "]"
   ],
   "id": "baaa89ce9b2530a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
